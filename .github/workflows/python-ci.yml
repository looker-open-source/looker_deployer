name: Python CI
on:
  pull_request:
    branches:
      - main

  push:
    branches:
      - main

  workflow_dispatch:

env:
  LOOKERSDK_BASE_URL: https://localhost:20000
  LOOKERSDK_VERIFY_SSL: false
  JUNIT_OUTPUT_DIR: results

jobs:
  unit:
    name: build-${{ matrix.python-version }}
    env:
      JUNIT_OUTPUT_NAME: ${{ matrix.os }}.py${{ matrix.python-version }}
    runs-on: ${{ matrix.os }}-latest

    strategy:
      # run all supported python versions on ubuntu
      # run only latest supported python version on windows/mac
      matrix:
        os:
          - ubuntu
        python-version:
          - '3.6'
          - '3.7'
          - '3.8'
          - '3.9'

    steps:
      - name: Repo Checkout
        uses: actions/checkout@v2

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v2
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          pip install flake8 flake8_formatter_junit_xml pytest pytest-mock
          pip install .

      - name: Lint with flake8
        run: |
          mkdir ${{ github.workspace }}/${JUNIT_OUTPUT_DIR}
          # stop the build if there are Python syntax errors or undefined names
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
          # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
          flake8 --format junit-xml . --exit-zero --max-complexity=10 \
            --max-line-length=127 \
            --output-file=${{ github.workspace }}/${JUNIT_OUTPUT_DIR}/flake8_${JUNIT_OUTPUT_NAME}.xml

      - name: Test with pytest
        run: |
          pytest --junitxml=${{ github.workspace }}/${JUNIT_OUTPUT_DIR}/pytest_${JUNIT_OUTPUT_NAME}.xml

      - name: Upload pytest test results
        if: ${{ always() }}
        uses: actions/upload-artifact@v2
        with:
          name: python-test-results
          path: results/

  publish-test-results:
    needs: unit
    if: success() || failure()
    runs-on: ubuntu-latest

    steps:
      - name: Download Artifacts
        uses: actions/download-artifact@v2
        with:
          path: artifacts

      # If there are no test result files the test is reported as passing!
      # That allows for some weird failure modes, where an early failure
      # is not distinguished from a pass.
      - name: Check Results Files Exist
        id: check-results-exist
        run: |
          if [[ $(ls -l artifacts/python-test-results/*.xml 2> /dev/null | wc -l) -ge "1" ]]; then
            exit 0
          else
            curl --request POST \
            --url ${{ github.api_url }}/repos/${{ github.repository }}/check-runs \
            --header "authorization: Bearer ${{ secrets.GITHUB_TOKEN }}" \
            --header "content-type: application/json" \
            --header "Accept: application/vnd.github.v3+json" \
            --data '{
              "name": "Python Tests",
              "head_sha": "${{ github.event.pull_request.head.sha || github.sha }}",
              "conclusion": "failure"
            }' \
            --fail \
            -o /dev/null \
            --silent
            exit 1
          fi

      - name: Publish Unit Test Results
        if: steps.check-results-exist.outcome == 'success'
        uses: EnricoMi/publish-unit-test-result-action@v1.12
        with:
          # Cosmetic issue with `check_name` being associated to the wrong
          # workflow: https://github.com/EnricoMi/publish-unit-test-result-action/issues/12
          check_name: Python Tests
          github_token: ${{ secrets.GITHUB_TOKEN }}
          report_individual_runs: true
          hide_comments: orphaned commits
          check_run_annotations_branch: '*'
          files: 'artifacts/python-test-results/*.xml'
